{"name":"NYC Subway Data Analysis","tagline":"Analysis and Regression modeling of NYC Subway dataset to check affect of rain on Subway ridership","body":"## Background\r\nIn this project, we look at the NYC Subway data and figure out if more people ride the subway when it is raining versus when it is not raining. We will wrangle the NYC subway data, use statistical methods and data visualization to draw an interesting conclusion about the subway dataset that we've analyzed.\r\n\r\n## Problems Encountered in the Map\r\nAfter initially downloading a small sample size of the Raleigh data and running it against a provisional data.py file, I noticed three main problems with the data, which I will discuss in the following order:\r\n* _Multiple streets for a node:_ A university like NCSU has multiple streets in it. But the street keys aren’t of the form “addr:street”. In such cases, the data is still cleaned for the streets but they are not added to an address dictionary value. Instead, the keys are kept as is.\r\n* _Relation nodes:_ There are some nodes who have element names as ‘Relation’. They are handled in a similar way as ways and nodes.\r\n* _Postal codes:_ There are a lot of postal codes which don’t pertain to Raleigh, NC. Some investigation is done around it to find out the root cause.\r\n\r\n\r\n## Data Overview\r\nThis section contains basic statistics about the dataset and the MongoDB queries used to gather them.\r\n                                                \r\n\r\n\r\n## Additional Ideas\r\n### **Contributor statistics and gamification suggestion**\r\nFew users contribute to the most data which suggests automated entry of some admin. Here are some statistics:\r\n* Top user contribution percentage (“jumbanho”) – 76.82%\r\n* Top user contribution percentage (“jumbanho”) – 76.82%\r\n\r\nIt is clear that the contribution of the top 10 users is too high as compared to the total number of users (724). If some incentives are given to the users to contribute more, it will help spur the data entry process and will also provide more quality to the data. \r\n\r\n### **Additional data exploration using MongoDB queries**\r\n\r\n## Linear Regression\r\nOLS using Statsmodels library was the approach used to perform linear regression. Gradient descent was not used as OLS is more accurate. Rain and Hour were the features used apart from the dummy variables which can be seen from the code below.\r\n```\r\nimport numpy as np\r\nimport pandas\r\nimport statsmodels.api as sm\r\n\r\ndef linear_regression(features, values):\r\n    \"\"\"\r\n    Perform linear regression given a data set with an arbitrary number of features.\r\n    \"\"\"\r\n    \r\n    features = sm.add_constant(features,prepend=True)\r\n    model = sm.OLS(values,features).fit()\r\n    print model.summary()\r\n    params = model.params\r\n    intercept = params[0]\r\n    params = params[1:]\r\n    return intercept, params\r\n\r\ndef predictions(dataframe):\r\n    features = dataframe[['rain', 'Hour']]\r\n    dummy_units = pandas.get_dummies(dataframe['UNIT'], prefix='unit')\r\n    features = features.join(dummy_units[['unit_R170','unit_R022','unit_R179','unit_R029','unit_R012',\r\n                            'unit_R046','unit_R023','unit_R047','unit_R293','unit_R055','unit_R084','unit_R011',\r\n                            'unit_R178','unit_R452','unit_R001','unit_R108',\r\n                            'unit_R010','unit_R195','unit_R021','unit_R102','unit_R018','unit_R033',\r\n                            'unit_R175','unit_R177','unit_R051','unit_R138','unit_R017','unit_R057',\r\n                            'unit_R168','unit_R080','unit_R050','unit_R127','unit_R025',\r\n                            'unit_R141','unit_R163','unit_R044','unit_R019','unit_R031','unit_R248','unit_R032',\r\n                            'unit_R083','unit_R461','unit_R240','unit_R235','unit_R208','unit_R014','unit_R053',\r\n                            'unit_R020','unit_R142','unit_R081','unit_R028','unit_R222','unit_R131','unit_R105',\r\n                            'unit_R113','unit_R144','unit_R167','unit_R079','unit_R463','unit_R035',\r\n                            'unit_R132','unit_R176','unit_R300','unit_R158','unit_R164','unit_R097',\r\n                            'unit_R101','unit_R116','unit_R027','unit_R110','unit_R201','unit_R160','unit_R041'\r\n                            ]])\r\n\r\n    # Values\r\n    values = dataframe['ENTRIESn_hourly']\r\n\r\n    # Perform linear regression\r\n    intercept, params = linear_regression(features, values)\r\n    \r\n    predictions = intercept + np.dot(features, params)\r\n    return predictions\r\n```\r\n\r\n### Choice of Predictors Explained\r\n* ‘rain’ is the first variable that comes to mind to decide whether people might use the subway or not. If it rains, then people might decide to use the subway. Also, ‘Hour’ of the day is another variable that helps in predicting usage of subway as people would use the subway more often in the mornings and the evenings as compared to late night or during the afternoon. All the dummy variables were included as they categorize the data and hence would help in identification. \r\n* To filter out the dummy variables, I ran a full model with all the dummy variables. The resulting model summary gave the t-values and p-values of the dummy variables. Inclusion of dummy variables with higher t-values improved the r-squared significantly. I sorted the variables in descending order of t-values and included dummy variables in the model till the r-squared value crossed the threshold mentioned of 0.4.\r\n\r\n### Non-dummy coefficient values and R^2\r\n**_Coefficients of the non-dummy variables:_**\r\n‘const’: -18.9345\r\n‘rain’: 62.6158\r\n‘hour’: 59.1683\r\n\r\n**_R^2 value of 0.401_** means that the model explained 40.1% of the data fit the regression model line. This linear model to predict ridership is not appropriate for this dataset, given the R^2 value. It could be that some variables given in the dataset which could increase the R^2 value have not been identified. There is also a chance that some important variable has not been captured during data collection too.\r\n\r\n## Visualizations\r\nA histogram of the hourly number of entries is plotted when there is rain and when there is not. The histograms are not normally distributed and have their peaks near 0. \r\n```\r\n    df = turnstile_weather.groupby('TIMEn')\r\n    dfSum = df['ENTRIESn_hourly'].sum()\r\n    \r\n    TIMEn = []\r\n    ENTRIESn_hourlySum = []\r\n    for key, value in dfSum.iteritems():\r\n        TIMEn.append(key)\r\n        ENTRIESn_hourlySum.append(value)\r\n    ENTRIESn_hourlySum = np.log(ENTRIESn_hourlySum)    \r\n    ENTRIESn_hourlySum = ENTRIESn_hourlySum.astype(int)\r\n    dfPlot = DataFrame({'TIMEn':Series(TIMEn),'ENTRIESn_hourlySum':Series(ENTRIESn_hourlySum)})    \r\n    #Filtering out the -inf values\r\n    dfPlot = dfPlot[dfPlot['ENTRIESn_hourlySum'] > 0]  \r\n    #Getting the highest ENTRIESn_hour sum values \r\n    dfPlot = dfPlot[dfPlot['ENTRIESn_hourlySum'] > (3*np.mean(dfPlot['ENTRIESn_hourlySum']))]    \r\n    plot = ggplot(dfPlot,aes('TIMEn','ENTRIESn_hourlySum')) + geom_bar(stat=\"bar\") + \\\r\n            ggtitle('Highest entry counts with times during the day') + xlab('Times in Day') + ylab('Log of Hourly Entries') \r\n    return plot\r\n```\r\n![Histogram of subway entry frequency v/s times](https://github.com/Inquisitive-Geek/Analysing-NYC-Subway-Dataset/blob/master/Images/Histogram.png?raw=true)\r\n\r\nThe below graph shows the plot of the mean pressure variation versus dates. It shows how the mean pressure variation varies on different days. \r\n```\r\n    pandas.options.mode.chained_assignment = None  # default='warn'\r\n    turnstile_weather.iscopy = False\r\n    turnstile_weather.loc[:,('DATEn')] = pandas.to_datetime(turnstile_weather.loc[:,('DATEn')], format=\"%Y-%m-%d\")\r\n    df = turnstile_weather.groupby('DATEn')\r\n    index = [gp_keys[0] for gp_keys in df.groups.values()]\r\n    unique_df = turnstile_weather.reindex(index).sort('DATEn')   \r\n    plot = ggplot(unique_df, aes(x='DATEn', y='meanpressurei')) + geom_line(color = 'red') + \\\r\n    ggtitle('Mean Pressure variation') + xlab('Date') + ylab('Mean Pressure') + scale_x_date(labels = date_format(\"%Y-%m-%d\"))\r\n    return plot\r\n```\r\n![Mean Pressure Variation v/s Date](https://github.com/Inquisitive-Geek/Analysing-NYC-Subway-Dataset/blob/master/Images/Mean_Pressure_Vs_Date.png?raw=true)\r\n\r\n## Conclusion\r\nMore people ride the NYC subway when it rains as compared to when it does not. The low two-sided p-value of 0.0498 indicates that the null hypothesis that the mean subway number of hourly entries with rain is equal to that without rain is rejected. The linear model indicates that rain is an important variable in deciding the number of hourly entries. The positive coefficient value of 63.64 for variable ‘rain’ shows that subway ridership increases by 63.64 on average when it rains.\r\n\r\n## Reflection on Shortcomings\r\nThe shortcomings of the methods used are as follows:\r\n* The data is not captured for all the hourly intervals for each of the unit types indicating that the data is not complete. This causes a hindrance in building reliable models for our analysis.\r\n* The linear regression model has too many dummy variables being used. Ideally, the number of variables should be lesser in a dataset. The issue is that removing a few of the dummy variables caused a reduction in the R-squared value which is not desirable.\r\n\r\n## Author\r\nThe above project was done as part of the 'Data Analayst Nanodegree' by Roshan Shetty. Please contact me on rosshanabshetty@gmail.com for any doubts, queries or information.","google":"UA-67244594-1","note":"Don't delete this file! It's used internally to help with page regeneration."}